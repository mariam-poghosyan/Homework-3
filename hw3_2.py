# -*- coding: utf-8 -*-
"""hw3#2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFsCJ-s9bA8HBVE6g5jXU6uBSgTJUw8u

Problem 2
"""

!pip install scrapy
import re
import time
import requests
import numpy as np
import pandas as pd
from scrapy.http import TextResponse

url = "https://staff.am/en/jobs"
base_url = "https://staff.am"

class Jobs:
    def __init__(self,url):
        self.url = url
        self.page = requests.get(self.url)
        self.response = TextResponse(body=self.page.text,url=self.url,encoding="utf-8")

    def get_vacancy(self):
        vac = self.response.xpath('//div[@class="job-inner job-item-title"]/p[@class="font_bold"]/text()').extract()
        return vac 

    def get_company(self):
        comp = self.response.xpath('//div[@class="job-inner job-item-title"]/p[@class="job_list_company_title"]/text()').extract()
        return comp

    def get_deadline(self):
        dl1 = self.response.css('div[class="job-inner job-list-deadline"] p::text').extract()
        dl2 = [''.join(x) for x in zip(dl1[0::2], dl1[1::2])]
        del dl2[1::2]
        dl = [i.replace("\n\n", "").replace("\n"," ").strip() for i in dl2]
        return dl 

    def get_location(self):
        loc = self.response.xpath('//div[@class="job-inner job-list-deadline"]/p[@class="job_location"]/text()').extract()
        loc = [i.replace('\n','').strip() for i in loc]
        return loc 

    def get_ind_page(self):
        ind_page = [base_url + i for i in self.response.xpath('//div[@class="list-view"]/div/div/a/@href').extract()]
        return ind_page

    def get_next(self):
        page = self.response.xpath('//ul[@class="pagination"]/li[@class="next"]/a/@href').extract()
        return page

j = Jobs(url)
vacancies = []
companies = []
deadlines = []
locations = []
i_pages = []
jobs = []

while True:
    if(j.get_next() == []):
        vacancies = vacancies + j.get_vacancy()
        companies = companies + j.get_company()
        deadlines = deadlines + j.get_deadline()
        locations = locations + j.get_location()
        i_pages = i_pages + j.get_ind_page()
        break
    else:
        vacancies = vacancies + j.get_vacancy()
        companies = companies + j.get_company()
        deadlines = deadlines + j.get_deadline()
        locations = locations + j.get_location()
        i_pages = i_pages + j.get_ind_page()
        url = base_url + j.get_next()[0]
        j = Jobs(url)

locations = [i for i in locations if len(i)>0]

jobs.append(vacancies)
jobs.append(companies)
jobs.append(deadlines)
jobs.append(locations)
jobs.append(i_pages)

jobs = list(map(list, zip(*jobs)))
jobs_df = pd.DataFrame(jobs, columns=['Vacancies','Companies','Deadlines','Locations','Individual Pages'])

from google.colab import files
jobs_df.to_csv('problem2.csv')
files.download('problem2.csv')

fj = jobs_df['Companies'].value_counts().idxmax()
fr_comp = []
for i in jobs_df['Companies']:
    if(i == fj):
        fr_comp.append(i)

print(fj)

jd = []
for i in jobs_df['Vacancies']:
    if("Data" in i or "DATA" in i):
        jd.append(i)

for i in jd:
    print(i)

